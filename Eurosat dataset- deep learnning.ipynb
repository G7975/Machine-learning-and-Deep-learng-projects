{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EuroSAT overview image](data/eurosat_overview.jpg)\n",
    "\n",
    "#### The EuroSAT dataset\n",
    "\n",
    "In this assignment, you will use the [EuroSAT dataset](https://github.com/phelber/EuroSAT). It consists of 27000 labelled Sentinel-2 satellite images of different land uses: residential, industrial, highway, river, forest, pasture, herbaceous vegetation, annual crop, permanent crop and sea/lake. For a reference, see the following papers:\n",
    "- Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Patrick Helber, Benjamin Bischke, Andreas Dengel, Damian Borth. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.\n",
    "- Introducing EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification. Patrick Helber, Benjamin Bischke, Andreas Dengel. 2018 IEEE International Geoscience and Remote Sensing Symposium, 2018.\n",
    "\n",
    "Your goal is to construct a neural network that classifies a satellite image into one of these 10 classes, as well as applying some of the saving and loading techniques you have learned in the previous sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data\n",
    "\n",
    "The dataset you will train your model on is a subset of the total data, with 4000 training images and 1000 testing images, with roughly equal numbers of each class. The code to import the data is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_eurosat_data():\n",
    "    data_dir = 'data/'\n",
    "    x_train = np.load(os.path.join(data_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n",
    "    x_test  = np.load(os.path.join(data_dir, 'x_test.npy'))\n",
    "    y_test  = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_eurosat_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now construct a model to fit to the data. Using the Sequential API, build your model according to the following specifications:\n",
    "\n",
    "* The model should use the input_shape in the function argument to set the input size in the first layer.\n",
    "* The first layer should be a Conv2D layer with 16 filters, a 3x3 kernel size, a ReLU activation function and 'SAME' padding. Name this layer 'conv_1'.\n",
    "* The second layer should also be a Conv2D layer with 8 filters, a 3x3 kernel size, a ReLU activation function and 'SAME' padding. Name this layer 'conv_2'.\n",
    "* The third layer should be a MaxPooling2D layer with a pooling window size of 8x8. Name this layer 'pool_1'.\n",
    "* The fourth layer should be a Flatten layer, named 'flatten'.\n",
    "* The fifth layer should be a Dense layer with 32 units, a ReLU activation. Name this layer 'dense_1'.\n",
    "* The sixth and final layer should be a Dense layer with 10 units and softmax activation. Name this layer 'dense_2'.\n",
    "\n",
    "In total, the network should have 6 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_new_model(input_shape):\n",
    "    \"\"\"\n",
    "    This function should build a Sequential model according to the above specification. Ensure the \n",
    "    weights are initialised by providing the input_shape argument in the first layer, given by the\n",
    "    function argument.\n",
    "    Your function should also compile the model with the Adam optimiser, sparse categorical cross\n",
    "    entropy loss function, and a single accuracy metric.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, kernel_size = (3,3), padding = 'SAME', activation = 'relu', name = 'cov_1', input_shape =(input_shape), ))\n",
    "    model.add(Conv2D(8, kernel_size = (3,3), padding = 'SAME', activation = 'relu', name = 'cov_2'))\n",
    "    model.add(MaxPooling2D(pool_size=(8,8), name= 'pool_1'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation = 'relu', name = 'dense_1'))\n",
    "    model.add(Dense(10, activation= 'softmax', name = 'dense_2'))\n",
    "    \n",
    "    \n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = get_new_model(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_test_accuracy(model, x_test, y_test):\n",
    "    \"\"\"Test model classification accuracy\"\"\"\n",
    "    test_loss, test_acc = model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "    print('accuracy: {acc:0.3f}'.format(acc=test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cov_1 (Conv2D)               (None, 64, 64, 16)        448       \n",
      "_________________________________________________________________\n",
      "cov_2 (Conv2D)               (None, 64, 64, 8)         1160      \n",
      "_________________________________________________________________\n",
      "pool_1 (MaxPooling2D)        (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 18,354\n",
      "Trainable params: 18,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "accuracy: 0.068\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.summary()\n",
    "get_test_accuracy(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create checkpoints to save model during training, with a criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_checkpoint_every_epoch():\n",
    "    \"\"\"\n",
    "    This function should return a ModelCheckpoint object that:\n",
    "    - saves the weights only at the end of every epoch\n",
    "    - saves into a directory called 'checkpoints_every_epoch' inside the current working directory\n",
    "    - generates filenames in that directory like 'checkpoint_XXX' where\n",
    "      XXX is the epoch number formatted to have three digits, e.g. 001, 002, 003, etc.\n",
    "    \"\"\"\n",
    "    checkpoint_every_epoch = ModelCheckpoint(filepath='checkpoints_every_epoch/checkpoint_{epoch}',\n",
    "                                            save_weights_only=True,\n",
    "                                            frequency = 'epochs',\n",
    "                                            )\n",
    "    \n",
    "    \n",
    "    return checkpoint_every_epoch\n",
    "    \n",
    "\n",
    "\n",
    "def get_checkpoint_best_only():\n",
    "    \"\"\"\n",
    "    This function should return a ModelCheckpoint object that:\n",
    "    - saves only the weights that generate the highest validation (testing) accuracy\n",
    "    - saves into a directory called 'checkpoints_best_only' inside the current working directory\n",
    "    - generates a file called 'checkpoints_best_only/checkpoint' \n",
    "    \"\"\"\n",
    "    checkpoints_best_only = ModelCheckpoint(filepath = 'checkpoints_best_only/checkpoint',\n",
    "                                           save_weights_only= True,\n",
    "                                           save_best_only= True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode = 'max',\n",
    "                                           verbose = False) \n",
    "    \n",
    "    \n",
    "    return checkpoints_best_only\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_early_stopping():\n",
    "    \"\"\"\n",
    "    This function should return an EarlyStopping callback that stops training when\n",
    "    the validation (testing) accuracy has not improved in the last 3 epochs.\n",
    "    HINT: use the EarlyStopping callback with the correct 'monitor' and 'patience'\n",
    "    \"\"\"\n",
    "    \n",
    "    earlystopping = EarlyStopping(monitor='val_accuracy',\n",
    "                                 patience=3)\n",
    "    \n",
    "    \n",
    "    return earlystopping\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_every_epoch = get_checkpoint_every_epoch()\n",
    "checkpoint_best_only = get_checkpoint_best_only()\n",
    "early_stopping = get_early_stopping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model using the callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 2.0257 - accuracy: 0.2282 - val_loss: 1.6489 - val_accuracy: 0.3910\n",
      "Epoch 2/50\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 1.4754 - accuracy: 0.4450 - val_loss: 1.4695 - val_accuracy: 0.4150\n",
      "Epoch 3/50\n",
      "4000/4000 [==============================] - 86s 22ms/sample - loss: 1.2923 - accuracy: 0.5110 - val_loss: 1.3408 - val_accuracy: 0.4830\n",
      "Epoch 4/50\n",
      "4000/4000 [==============================] - 85s 21ms/sample - loss: 1.2297 - accuracy: 0.5495 - val_loss: 1.2494 - val_accuracy: 0.5340\n",
      "Epoch 5/50\n",
      "4000/4000 [==============================] - 85s 21ms/sample - loss: 1.1490 - accuracy: 0.5845 - val_loss: 1.2361 - val_accuracy: 0.5570\n",
      "Epoch 6/50\n",
      "4000/4000 [==============================] - 86s 21ms/sample - loss: 1.0674 - accuracy: 0.6225 - val_loss: 1.1738 - val_accuracy: 0.5700\n",
      "Epoch 7/50\n",
      "4000/4000 [==============================] - 86s 22ms/sample - loss: 1.0132 - accuracy: 0.6435 - val_loss: 1.0721 - val_accuracy: 0.6260\n",
      "Epoch 8/50\n",
      "4000/4000 [==============================] - 86s 22ms/sample - loss: 0.9632 - accuracy: 0.6505 - val_loss: 1.0636 - val_accuracy: 0.5930\n",
      "Epoch 9/50\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.9114 - accuracy: 0.6787 - val_loss: 0.9680 - val_accuracy: 0.6550\n",
      "Epoch 10/50\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.8665 - accuracy: 0.6920 - val_loss: 0.9403 - val_accuracy: 0.6670\n",
      "Epoch 11/50\n",
      "4000/4000 [==============================] - 92s 23ms/sample - loss: 0.8097 - accuracy: 0.7145 - val_loss: 0.9208 - val_accuracy: 0.6680\n",
      "Epoch 12/50\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.7796 - accuracy: 0.7225 - val_loss: 0.9681 - val_accuracy: 0.6560\n",
      "Epoch 13/50\n",
      "4000/4000 [==============================] - 93s 23ms/sample - loss: 0.7550 - accuracy: 0.7322 - val_loss: 0.8802 - val_accuracy: 0.6730\n",
      "Epoch 14/50\n",
      "4000/4000 [==============================] - 85s 21ms/sample - loss: 0.7101 - accuracy: 0.7533 - val_loss: 0.8906 - val_accuracy: 0.6810\n",
      "Epoch 15/50\n",
      "4000/4000 [==============================] - 85s 21ms/sample - loss: 0.7089 - accuracy: 0.7442 - val_loss: 0.9018 - val_accuracy: 0.6670\n",
      "Epoch 16/50\n",
      "4000/4000 [==============================] - 86s 21ms/sample - loss: 0.6913 - accuracy: 0.7533 - val_loss: 0.8590 - val_accuracy: 0.6840\n",
      "Epoch 17/50\n",
      "4000/4000 [==============================] - 85s 21ms/sample - loss: 0.6649 - accuracy: 0.7650 - val_loss: 0.8371 - val_accuracy: 0.6930\n",
      "Epoch 18/50\n",
      "4000/4000 [==============================] - 86s 22ms/sample - loss: 0.6676 - accuracy: 0.7573 - val_loss: 0.8201 - val_accuracy: 0.6970\n",
      "Epoch 19/50\n",
      "4000/4000 [==============================] - 84s 21ms/sample - loss: 0.6430 - accuracy: 0.7732 - val_loss: 0.8674 - val_accuracy: 0.6860\n",
      "Epoch 20/50\n",
      "4000/4000 [==============================] - 86s 22ms/sample - loss: 0.6119 - accuracy: 0.7782 - val_loss: 0.8572 - val_accuracy: 0.6890\n",
      "Epoch 21/50\n",
      "4000/4000 [==============================] - 89s 22ms/sample - loss: 0.5911 - accuracy: 0.7887 - val_loss: 0.7529 - val_accuracy: 0.7240\n",
      "Epoch 22/50\n",
      "4000/4000 [==============================] - 86s 22ms/sample - loss: 0.5844 - accuracy: 0.7928 - val_loss: 0.8287 - val_accuracy: 0.7090\n",
      "Epoch 23/50\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.5712 - accuracy: 0.7987 - val_loss: 0.7720 - val_accuracy: 0.7270\n",
      "Epoch 24/50\n",
      "4000/4000 [==============================] - 84s 21ms/sample - loss: 0.5631 - accuracy: 0.7968 - val_loss: 0.8313 - val_accuracy: 0.7050\n",
      "Epoch 25/50\n",
      "4000/4000 [==============================] - 89s 22ms/sample - loss: 0.5633 - accuracy: 0.7960 - val_loss: 0.7862 - val_accuracy: 0.7300\n",
      "Epoch 26/50\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.5240 - accuracy: 0.8112 - val_loss: 0.7692 - val_accuracy: 0.7320\n",
      "Epoch 27/50\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.5248 - accuracy: 0.8112 - val_loss: 0.9144 - val_accuracy: 0.6890\n",
      "Epoch 28/50\n",
      "4000/4000 [==============================] - 89s 22ms/sample - loss: 0.5253 - accuracy: 0.8123 - val_loss: 0.7722 - val_accuracy: 0.7250\n",
      "Epoch 29/50\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.5110 - accuracy: 0.8175 - val_loss: 0.8252 - val_accuracy: 0.7150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff2dcc919e8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "callbacks = [checkpoint_every_epoch, checkpoint_best_only, early_stopping]\n",
    "model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new instance of model and load on both sets of weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6.4M\r\n",
      "-rw-r--r-- 1 jovyan users   83 Dec 29 17:30 checkpoint\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:02 checkpoint_10.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:02 checkpoint_10.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:04 checkpoint_11.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:04 checkpoint_11.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:05 checkpoint_12.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:05 checkpoint_12.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:07 checkpoint_13.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:07 checkpoint_13.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:08 checkpoint_14.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:08 checkpoint_14.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:10 checkpoint_15.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:10 checkpoint_15.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:11 checkpoint_16.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:11 checkpoint_16.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:13 checkpoint_17.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:13 checkpoint_17.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:14 checkpoint_18.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:14 checkpoint_18.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:15 checkpoint_19.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:15 checkpoint_19.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 16:49 checkpoint_1.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 16:49 checkpoint_1.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:17 checkpoint_20.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:17 checkpoint_20.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:18 checkpoint_21.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:18 checkpoint_21.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:20 checkpoint_22.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:20 checkpoint_22.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:21 checkpoint_23.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:21 checkpoint_23.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:23 checkpoint_24.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:23 checkpoint_24.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:24 checkpoint_25.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:24 checkpoint_25.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:26 checkpoint_26.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:26 checkpoint_26.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:27 checkpoint_27.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:27 checkpoint_27.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:29 checkpoint_28.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:29 checkpoint_28.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:30 checkpoint_29.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:30 checkpoint_29.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 16:51 checkpoint_2.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 16:51 checkpoint_2.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 16:52 checkpoint_3.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 16:52 checkpoint_3.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 16:54 checkpoint_4.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 16:54 checkpoint_4.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 16:55 checkpoint_5.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 16:55 checkpoint_5.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 16:57 checkpoint_6.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 16:57 checkpoint_6.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 16:58 checkpoint_7.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 16:58 checkpoint_7.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 16:59 checkpoint_8.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 16:59 checkpoint_8.index\r\n",
      "-rw-r--r-- 1 jovyan users 219K Dec 29 17:01 checkpoint_9.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec 29 17:01 checkpoint_9.index\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh checkpoints_every_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model_last_epoch(model):\n",
    "    \"\"\"\n",
    "    This function should create a new instance of the CNN you created earlier,\n",
    "    load on the weights from the last training epoch, and return this model.\n",
    "    \"\"\"\n",
    "    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir='checkpoints_every_epoch'))\n",
    "    \n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def get_model_best_epoch(model):\n",
    "    \"\"\"\n",
    "    This function should create a new instance of the CNN you created earlier, load \n",
    "    on the weights leading to the highest validation accuracy, and return this model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.load_weights('checkpoints_best_only/checkpoint')\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with last epoch weights:\n",
      "accuracy: 0.715\n",
      "\n",
      "Model with best epoch weights:\n",
      "accuracy: 0.732\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_last_epoch = get_model_last_epoch(get_new_model(x_train[0].shape))\n",
    "model_best_epoch = get_model_best_epoch(get_new_model(x_train[0].shape))\n",
    "print('Model with last epoch weights:')\n",
    "get_test_accuracy(model_last_epoch, x_test, y_test)\n",
    "print('')\n",
    "print('Model with best epoch weights:')\n",
    "get_test_accuracy(model_best_epoch, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load, from scratch, a model trained on the EuroSat dataset.\n",
    "\n",
    "In your workspace, you will find another model trained on the `EuroSAT` dataset in `.h5` format. This model is trained on a larger subset of the EuroSAT dataset and has a more complex architecture. The path to the model is `models/EuroSatNet.h5`. See how its testing accuracy compares to your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model_eurosatnet():\n",
    "    \"\"\"\n",
    "    This function should return the pretrained EuroSatNet.h5 model.\n",
    "    \"\"\"\n",
    "    model = load_model('models/EuroSatNet.h5')\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_1 (Conv2D)              (None, 64, 64, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 64, 64, 16)        6416      \n",
      "_________________________________________________________________\n",
      "pool_1 (MaxPooling2D)        (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv2D)              (None, 32, 32, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv_4 (Conv2D)              (None, 32, 32, 16)        6416      \n",
      "_________________________________________________________________\n",
      "pool_2 (MaxPooling2D)        (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv_5 (Conv2D)              (None, 16, 16, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv_6 (Conv2D)              (None, 16, 16, 16)        6416      \n",
      "_________________________________________________________________\n",
      "pool_3 (MaxPooling2D)        (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv_7 (Conv2D)              (None, 8, 8, 16)          2320      \n",
      "_________________________________________________________________\n",
      "conv_8 (Conv2D)              (None, 8, 8, 16)          6416      \n",
      "_________________________________________________________________\n",
      "pool_4 (MaxPooling2D)        (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 41,626\n",
      "Trainable params: 41,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "accuracy: 0.810\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_eurosatnet = get_model_eurosatnet()\n",
    "model_eurosatnet.summary()\n",
    "get_test_accuracy(model_eurosatnet, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "tensor-flow-2-1",
   "graded_item_id": "JaRY0",
   "launcher_item_id": "mJ8fg"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
